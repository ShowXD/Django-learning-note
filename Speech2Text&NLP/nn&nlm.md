# NN and NLM

## The basic
### perceptron
`他的activation function是non linear function`  
![image](https://github.com/ShowXD/Learning-note/assets/29877260/b1024c98-cffb-4f80-82f6-d5cef18c5402)

### rectified
` `  
![image](https://github.com/ShowXD/Learning-note/assets/29877260/1eb411b1-0c68-4008-b94d-ef05c227c8e9)

## Backprop algorithm
`相通東西的步驟分一樣`

### forward pass
### backward pass
### gpu speed up
* large width $(D^{(k)})$ at each layer
* larger batch size
* __more deep network__
